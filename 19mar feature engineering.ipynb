{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d3b9363-c6ca-4874-b4cf-fc597123ca5d",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21deed94-0431-4970-aec8-3bf5cf0a5ee2",
   "metadata": {},
   "source": [
    "Min-Max scaling is a data preprocessing technique used to transform the values of numerical features to a common scale. It works by rescaling the range of the feature values to a new range, typically [0, 1]. This is done by subtracting the minimum value of the feature from each observation and then dividing by the range of the feature values.\n",
    "\n",
    "For example, suppose we have a dataset with a numerical feature Age that ranges from 20 to 80. Using Min-Max scaling, we can transform the values of this feature to the range [0, 1] using the following formula:\n",
    "\n",
    "Scaled_Age = (Age - min(Age)) / (max(Age) - min(Age))\n",
    "\n",
    "where min(Age) and max(Age) are the minimum and maximum values of the Age feature, respectively.\n",
    "\n",
    "After applying this transformation, an observation with an Age value of 20 would have a Scaled_Age value of 0, while an observation with an Age value of 80 would have a Scaled_Age value of 1.\n",
    "\n",
    "Min-Max scaling is commonly used in data preprocessing to bring all numerical features to a common scale before applying machine learning algorithms. This can help improve the performance of some algorithms that are sensitive to the scale of the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e858ec01-2ea7-4f99-b92e-1f394f29f699",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e1886e-aa4c-4265-99a8-3b81b19471b7",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as normalization or L2 normalization, is a feature scaling method that rescales the values of a feature vector so that it has a unit norm. This is done by dividing each value in the feature vector by the Euclidean norm (L2 norm) of the vector.\n",
    "\n",
    "For example, suppose we have a dataset with a numerical feature Age with values [20, 30, 40]. The Euclidean norm of this feature vector is calculated as sqrt(20^2 + 30^2 + 40^2) = 53.85. Using the Unit Vector technique, we can normalize the values of this feature by dividing each value by the Euclidean norm: [20/53.85, 30/53.85, 40/53.85] = [0.37, 0.56, 0.74].\n",
    "\n",
    "The main difference between the Unit Vector technique and Min-Max scaling is that the Unit Vector technique rescales the values of a feature vector so that it has a unit norm, while Min-Max scaling rescales the range of the feature values to a new range, typically [0, 1].\n",
    "\n",
    "The Unit Vector technique is commonly used in text classification and clustering problems where the data is represented as high-dimensional sparse vectors. It can help improve the performance of some machine learning algorithms by ensuring that all features have the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb297fb9-c007-46d7-bf95-5afeb25327d3",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa65964-be13-4de4-9dd0-f823a4797da6",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a statistical technique used for dimensionality reduction. It works by identifying patterns in the data and transforming the original variables into a new set of uncorrelated variables called principal components. These principal components capture most of the variance in the data and can be used to represent the original data in a lower-dimensional space.\n",
    "\n",
    "For example, suppose we have a dataset with two numerical features X and Y that are highly correlated. Using PCA, we can transform these two features into a single principal component that captures most of the variance in the data. This principal component can then be used to represent the original data in a lower-dimensional space.\n",
    "\n",
    "PCA is commonly used in data preprocessing to reduce the dimensionality of high-dimensional datasets before applying machine learning algorithms. This can help improve the performance of some algorithms by reducing the number of input features and removing noise and redundancy from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c00fee-f9ea-4578-afd5-9b5d1240e3b3",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1425ab-2384-461a-a0fb-f7fe2461e929",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a technique that can be used for both dimensionality reduction and feature extraction. Feature extraction involves transforming the original variables into a new set of variables that are more informative and easier to work with. PCA achieves this by identifying patterns in the data and transforming the original variables into a new set of uncorrelated variables called principal components.\n",
    "\n",
    "For example, suppose we have a dataset with two numerical features X and Y that are highly correlated. Using PCA, we can transform these two features into a single principal component that captures most of the variance in the data. This principal component can be considered a new feature that represents the original data in a more informative and compact way.\n",
    "\n",
    "In this way, PCA can be used for feature extraction by transforming the original variables into a new set of principal components that are more informative and easier to work with. These principal components can then be used as input features for machine learning algorithms or other data analysis tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32bbd6f-c091-485b-97f5-e23f655ac516",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a30a8d3-94ce-4d30-8a54-343d9f6bc584",
   "metadata": {},
   "source": [
    "To use Min-Max scaling to preprocess the data for a recommendation system for a food delivery service, I would follow these steps:\n",
    "\n",
    "Identify the numerical feature - In this case, the numerical features are price, rating, and delivery time.\n",
    "\n",
    "Calculate the minimum and maximum value for each feature- For each numerical feature, I would calculate the minimum and maximum values in the dataset.\n",
    "\n",
    "Rescale the value of each feature- For each numerical feature, I would rescale the values to the range [0, 1] using the following formula:\n",
    "\n",
    "Scaled_Value = (Value - min(Value)) / (max(Value) - min(Value))\n",
    "\n",
    "where min(Value) and max(Value) are the minimum and maximum values of the feature, respectively.\n",
    "\n",
    "After applying this transformation, all numerical features will have values in the range [0, 1]. This can help improve the performance of some machine learning algorithms by ensuring that all features have the same scale.\n",
    "\n",
    "Itâ€™s important to note that Min-Max scaling can be sensitive to outliers. If there are extreme values in the dataset, it may be necessary to apply some form of outlier detection or removal before applying Min-Max scaling.\n",
    "\n",
    "Received message. To use Min-Max scaling to preprocess the data for a recommendation system for a food delivery service, I would follow these steps: 1. **Identify the numerical features**: In this case, the numerical features are price, rating, and delivery time. 2. **Calculate the minimum and maximum values for each feature**: For each numerical feature, I would calculate the minimum and maximum values in the dataset. 3. **Rescale the values of each feature**: For each numerical feature, I would rescale the values to the range [0, 1] using the following formula: `Scaled_Value = (Value - min(Value)) / (max(Value) - min(Value))` where `min(Value)` and `max(Value)` are the minimum and maximum values of the feature, respectively. After applying this transformation, all numerical features will have values in the range [0, 1]. This can help improve the performance of some machine learning algorithms by ensuring that all features have the same scale. It's important to note that Min-Max scaling can be sensitive to outliers. If there are extreme values in the dataset, it may be necessary to apply some form of outlier detection or removal before applying Min-Max scaling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d5da32-efd0-41d8-8001-c7d8e1281724",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ad077b-53bc-42af-970f-49d7d774f4e1",
   "metadata": {},
   "source": [
    "To use PCA (Principal Component Analysis) to reduce the dimensionality of a dataset for predicting stock prices, I would follow these steps:\n",
    "\n",
    "Standardize the data: Before applying PCA, it is important to standardize the data to ensure that all features have the same scale. This can be done by subtracting the mean and dividing by the standard deviation for each feature.\n",
    "\n",
    "Calculate the covariance matrix: PCA works by identifying patterns in the data and transforming the original variables into a new set of uncorrelated variables called principal components. To do this, I would calculate the covariance matrix of the standardized data.\n",
    "\n",
    "Calculate the eigenvectors and eigenvalues: The next step is to calculate the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the direction of the principal components, while the eigenvalues represent their magnitude.\n",
    "\n",
    "Choose the number of principal components: Based on the eigenvalues, I would choose the number of principal components to retain. Typically, only the principal components with the largest eigenvalues are retained, as they capture most of the variance in the data.\n",
    "\n",
    "Transform the data: Finally, I would transform the original data into the new coordinate system defined by the chosen principal components. This can be done by multiplying the standardized data by the matrix of eigenvectors corresponding to the chosen principal components.\n",
    "\n",
    "After applying this transformation, the original high-dimensional data will be represented in a lower-dimensional space defined by the chosen principal components. This can help improve the performance of some machine learning algorithms by reducing the number of input features and removing noise and redundancy from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e5ed60-b7db-4b3d-9362-b499612120fd",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a1f726-68e9-41b3-8e75-4e1cb7089028",
   "metadata": {},
   "source": [
    "Scaled_Value = ((Value - min(Value)) / (max(Value) - min(Value))) * (new_max - new_min) + new_min\n",
    "\n",
    "where min(Value) and max(Value) are the minimum and maximum values of the dataset, respectively, and new_min and new_max are the desired minimum and maximum values of the scaled data.\n",
    "\n",
    "In this case, min(Value) = 1, max(Value) = 20, new_min = -1, and new_max = 1. Plugging these values into the formula above, we get:\n",
    "\n",
    "Scaled_Value = ((Value - 1) / (20 - 1)) * (1 - (-1)) + (-1)\n",
    "\n",
    "Applying this transformation to each value in the dataset, we get:\n",
    "\n",
    "Scaled_1 = ((1 - 1) / (20 - 1)) * (1 - (-1)) + (-1) = -1\n",
    "\n",
    "Scaled_5 = ((5 - 1) / (20 - 1)) * (1 - (-1)) + (-1) = -0.6\n",
    "\n",
    "Scaled_10 = ((10 - 1) / (20 - 1)) * (1 - (-1)) + (-1) = -0.2\n",
    "\n",
    "Scaled_15 = ((15 - 1) / (20 - 1)) * (1 - (-1)) + (-1) = 0.2\n",
    "\n",
    "Scaled_20 = ((20 - 1) / (20 - 1)) * (1 - (-1)) + (-1) = 0.999\n",
    "\n",
    "Therefore, after applying Min-Max scaling to transform the values to a range of -1 to 1, the scaled dataset is [-1, -0.6, -0.2, 0.2, 0.999]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24f1c5d-121f-41dc-8a19-347e34791ca4",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c97485-582c-4af5-84c2-40857345f0eb",
   "metadata": {},
   "source": [
    "Standardize the data: Before applying PCA, it is important to standardize the data to ensure that all features have the same scale. This can be done by subtracting the mean and dividing by the standard deviation for each feature.\n",
    "\n",
    "Calculate the covariance matrix: PCA works by identifying patterns in the data and transforming the original variables into a new set of uncorrelated variables called principal components. To do this, I would calculate the covariance matrix of the standardized data.\n",
    "\n",
    "Calculate the eigenvectors and eigenvalues: The next step is to calculate the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the direction of the principal components, while the eigenvalues represent their magnitude.\n",
    "\n",
    "Choose the number of principal components: Based on the eigenvalues, I would choose the number of principal components to retain. Typically, only the principal components with the largest eigenvalues are retained, as they capture most of the variance in the data.\n",
    "\n",
    "The number of principal components to retain depends on how much variance in the data we want to capture. A common approach is to choose enough principal components to capture at least 95% of the variance in the data. This can be done by calculating the cumulative sum of the eigenvalues and choosing enough principal components to reach this threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7667e870-dc9b-42b7-8d15-0d69bf46d63b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
