{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1909541-e865-428a-bec4-b64f9ebe2f05",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7184ec8a-df63-46b1-a5dd-99d2e2192535",
   "metadata": {},
   "source": [
    "To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we can use conditional probability. We are looking for the conditional probability of being a smoker (S) given that the employee uses the health insurance plan (H), denoted as \n",
    "\n",
    "P(S∣H).\n",
    "\n",
    "We are given the following information:\n",
    "\n",
    "�\n",
    "P(H)=0.70 (probability of using the health insurance plan)\n",
    "\n",
    "P(S∣H)=0.40 (probability of being a smoker given that the employee uses the plan)\n",
    "We can use Bayes' theorem to calculate \n",
    "\n",
    "P(S∣H):\n",
    "\n",
    "\n",
    "P(S∣H)= \n",
    "P(H)\n",
    "P(H∣S)⋅P(S)\n",
    "​\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "\n",
    "P(H∣S) is the probability of using the health insurance plan given that the employee is a smoker. We can calculate this as \n",
    "1using the plan|smoker\n",
    "\n",
    "1−P(not using the plan|smoker).\n",
    "\n",
    "P(S) is the overall probability of being a smoker.\n",
    "Let's calculate it step by step:\n",
    "\n",
    "Calculate \n",
    "\n",
    "P(H∣S), the probability of using the health insurance plan given that the employee is a smoker:\n",
    "\n",
    "\n",
    "not using the plan|smoker\n",
    ")\n",
    "P(H∣S)=1−P(not using the plan|smoker)\n",
    "\n",
    "not using the plan|smoker\n",
    "\n",
    "using the plan|smoker\n",
    ")\n",
    "P(not using the plan|smoker)=1−P(using the plan|smoker)\n",
    "\n",
    "using the plan|smoker\n",
    "\n",
    "P(using the plan|smoker)=0.40 (given)\n",
    "So, \n",
    "\n",
    "P(H∣S)=1−0.40=0.60.\n",
    "\n",
    "Calculate \n",
    "\n",
    "P(S), the overall probability of being a smoker:\n",
    "\n",
    "\n",
    "not using the plan\n",
    "\n",
    "not using the plan\n",
    "\n",
    "P(S)=P(H)⋅P(S∣H)+P(not using the plan)⋅P(S∣not using the plan)\n",
    "We are not given \n",
    "\n",
    "not using the plan\n",
    "\n",
    "P(not using the plan) or \n",
    "\n",
    "not using the plan\n",
    "\n",
    "P(S∣not using the plan), so we cannot calculate \n",
    "\n",
    "P(S) without additional information.\n",
    "Without knowing the probability of not using the plan (\n",
    "\n",
    "not using the plan\n",
    "\n",
    "P(not using the plan)) and the probability of being a smoker given not using the plan (\n",
    "\n",
    "not using the plan\n",
    "\n",
    "P(S∣not using the plan)), we cannot determine the exact value of \n",
    "\n",
    "P(S). We need more information to complete the calculation.\n",
    "\n",
    "However, if you have additional information about the proportion of employees who do not use the plan and the proportion of non-smokers among those who do not use the plan, you can use that information to calculate \n",
    "\n",
    "P(S) and then find \n",
    "\n",
    "P(S∣H) using Bayes' theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a638212-0262-4056-bde1-8d0a7da32069",
   "metadata": {},
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa70c7a-7540-4140-8865-b0e0e83d0934",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are two different variants of the Naive Bayes algorithm used for text classification and other types of discrete data classification. The primary difference between them lies in the type of data they are designed to work with:\n",
    "\n",
    "Bernoulli Naive Bayes:\n",
    "\n",
    "Data Type: Bernoulli Naive Bayes is suitable for binary or Boolean data, where features are either present (1) or absent (0). It assumes that features are conditionally independent given the class.\n",
    "\n",
    "Example: It is often used in text classification tasks when binary features represent the presence or absence of specific words or features.\n",
    "\n",
    "Usage: Bernoulli Naive Bayes is appropriate when you are dealing with binary data, such as document classification where you want to know if certain keywords are present in a document or not.\n",
    "\n",
    "Multinomial Naive Bayes:\n",
    "\n",
    "Data Type: Multinomial Naive Bayes is appropriate for discrete data, particularly when dealing with text data or features that represent counts or frequencies. It assumes that features are conditionally independent given the class and follows a multinomial distribution.\n",
    "\n",
    "Example: It is widely used in text classification tasks, such as spam detection, sentiment analysis, and document categorization, where features are often word counts or term frequencies.\n",
    "\n",
    "Usage: Multinomial Naive Bayes is suitable when you are dealing with data that represents counts or frequencies, such as word counts in a document or the occurrence of words in a bag-of-words representation.\n",
    "\n",
    "In summary, the choice between Bernoulli Naive Bayes and Multinomial Naive Bayes depends on the nature of your data:\n",
    "\n",
    "Use Bernoulli Naive Bayes for binary data, where features are either present or absent, such as presence/absence of specific keywords in text.\n",
    "\n",
    "Use Multinomial Naive Bayes for discrete data, particularly in text classification tasks where features represent counts or frequencies, such as word counts in documents or term frequencies in text data.\n",
    "\n",
    "Both algorithms are variants of Naive Bayes that make different assumptions about the distribution of the data and can be suitable for different types of classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5585412a-cbf6-4837-b9bb-6a93a30b4f06",
   "metadata": {},
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7fd073-57d1-4a1b-98bf-97f25a25c04c",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes, like other Naive Bayes variants, generally assumes that missing values are not explicitly handled but are considered as one of the possible values of a feature. In Bernoulli Naive Bayes, missing values are typically treated as one of the two possible binary values (0 or 1) for a feature, depending on the application and the specific handling of missing data.\n",
    "\n",
    "Here are a few common approaches to handling missing values with Bernoulli Naive Bayes:\n",
    "\n",
    "Imputation:\n",
    "\n",
    "One common approach is to impute missing values with a specific value, such as 0 or 1, depending on the context.\n",
    "For example, in text classification tasks where features represent the presence or absence of words, missing values may be imputed as 0 (absence of the word) for simplicity.\n",
    "\n",
    "Handling as a Separate Category:\n",
    "\n",
    "In some cases, missing values are treated as a separate category or class within the feature.\n",
    "For example, if you are classifying documents based on the presence of specific keywords and some keywords are missing from certain documents, you might treat the absence of a keyword as a separate category and assign it a value like \"2\" to distinguish it from \"0\" (absence) and \"1\" (presence).\n",
    "\n",
    "Dropping Instances:\n",
    "\n",
    "In situations where missing values are not informative or too common, you may choose to exclude instances (data points) with missing values from the analysis.\n",
    "\n",
    "This approach should be used with caution, as it can lead to a loss of information and potentially biased results if there is a systematic reason for the missing values.\n",
    "\n",
    "The choice of how to handle missing values in Bernoulli Naive Bayes depends on the specific problem, the domain, and the dataset. It's important to consider the implications of the chosen approach on the model's performance and the interpretation of results. Additionally, it's a good practice to carefully document how missing values are treated in your analysis to ensure transparency and reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697b8f21-c5f7-4c26-81ed-4c05f6685836",
   "metadata": {},
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb26ce2f-e3b7-4d4b-bac2-dd053a88e7ac",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is a variant of the Naive Bayes algorithm that is particularly suitable for data that follows a Gaussian (normal) distribution. It is most commonly used for binary and multi-class classification tasks.\n",
    "\n",
    "In Gaussian Naive Bayes, the assumption is that the continuous features in each class are normally distributed. When applied to multi-class classification, the algorithm models the distribution of each class's features separately and calculates the probability of an instance belonging to each class based on the Gaussian probability density function.\n",
    "\n",
    "Here's how Gaussian Naive Bayes can be used for multi-class classification:\n",
    "\n",
    "Model Training:\n",
    "\n",
    "For each class in the multi-class problem, the algorithm estimates the mean and variance of each feature's distribution within that class.\n",
    "\n",
    "It calculates the class-specific Gaussian probability density function (PDF) for each feature.\n",
    "Classification:\n",
    "\n",
    "When a new instance is to be classified, the algorithm calculates the likelihood of the instance's feature values given each class's Gaussian distribution.\n",
    "\n",
    "It then combines these likelihoods with the prior probabilities of each class to compute the posterior probabilities for each class.\n",
    "The class with the highest posterior probability is predicted as the class label for the instance.\n",
    "\n",
    "Gaussian Naive Bayes can handle multi-class problems naturally by extending the basic Naive Bayes framework. Each class is treated as a separate category, and the algorithm independently models the distribution of features within each class. The decision is made based on which class has the highest posterior probability.\n",
    "\n",
    "It's important to note that while Gaussian Naive Bayes can be applied to multi-class problems, its performance may vary depending on the data and the assumptions made about the distribution of features. In cases where the data doesn't follow a Gaussian distribution or when there are strong dependencies between features, other algorithms like Multinomial Naive Bayes or Decision Trees may be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c03481-036c-4f64-9b14-d0cb72aa39ea",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427915a5-4036-414b-b731-f65ecb806d41",
   "metadata": {},
   "source": [
    "When dealing with imbalanced classes in a classification model, accuracy may not be the most informative metric, as it can be misleading. In such cases, it's often more appropriate to consider metrics that provide a more comprehensive evaluation of the model's performance. The choice of metric depends on the specific goals and requirements of your application.\n",
    "\n",
    "Precision, recall, and F1 score are commonly used metrics for evaluating classification models with imbalanced classes:\n",
    "\n",
    "Precision: Precision is the ratio of correctly predicted positive observations to the total predicted positives. It is a measure of how many of the predicted positive instances are actually relevant. High precision indicates a low false positive rate.\n",
    "\n",
    "Precision\n",
    "=\n",
    "True Positives\n",
    "True Positives + False Positives\n",
    "Precision= \n",
    "True Positives + False Positives\n",
    "True Positives\n",
    "​\n",
    " \n",
    "\n",
    "Recall (Sensitivity or True Positive Rate): Recall is the ratio of correctly predicted positive observations to the all observations in the actual class. It is a measure of how many of the actual positive instances were correctly predicted. High recall indicates a low false negative rate.\n",
    "\n",
    "Recall\n",
    "=\n",
    "True Positives\n",
    "True Positives + False Negatives\n",
    "Recall= \n",
    "True Positives + False Negatives\n",
    "True Positives\n",
    "​\n",
    " \n",
    "\n",
    "F1 Score: The F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall, making it suitable for imbalanced datasets. The F1 score is especially useful when you want to consider both false positives and false negatives.\n",
    "\n",
    "F1 Score\n",
    "=\n",
    "2\n",
    "×\n",
    "(\n",
    "Precision\n",
    "×\n",
    "Recall\n",
    "Precision + Recall\n",
    ")\n",
    "F1 Score=2×( \n",
    "Precision + Recall\n",
    "Precision×Recall\n",
    "​\n",
    " )\n",
    "\n",
    "In summary, when dealing with imbalanced classes, it's often recommended to use precision, recall, or F1 score rather than accuracy alone. The choice between precision, recall, and F1 score depends on the specific requirements of your problem. If you want to balance false positives and false negatives, F1 score might be a good choice. If one type of error is more critical than the other, you may prioritize either precision or recall accordingly.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b18472-f1e1-45e5-ae71-da6f6e7bad22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
