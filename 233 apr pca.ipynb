{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9f80e53-cc8a-45ff-a3ac-a8deca636aef",
   "metadata": {},
   "source": [
    "Q1. What is the curse of dimensionality reduction and why is it important in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ebbe77-24fe-4b23-ad92-0aa1c21d9fd3",
   "metadata": {},
   "source": [
    "The \"curse of dimensionality\" refers to various challenges and phenomena that arise when dealing with high-dimensional data in machine learning. As the number of features or dimensions increases, the amount of data required to adequately cover the space grows exponentially, leading to several issues. Here are some key aspects of the curse of dimensionality and its importance in machine learning:\n",
    "\n",
    "Sparsity of Data:\n",
    "\n",
    "In high-dimensional spaces, the data points become sparser, and the available data might not be representative enough to effectively characterize the underlying distribution.\n",
    "Sparse data can lead to overfitting, where a model performs well on training data but fails to generalize to new, unseen data.\n",
    "Increased Computational Complexity:\n",
    "\n",
    "High-dimensional datasets require more computational resources and time for training machine learning models.\n",
    "Algorithms that rely on distance computations, such as K-nearest neighbors (KNN), may become less efficient as the number of dimensions increases.\n",
    "Difficulty in Visualization:\n",
    "\n",
    "Visualizing data in high-dimensional spaces becomes challenging or impossible for humans.\n",
    "Understanding relationships and patterns within the data becomes difficult, hindering the interpretability of models.\n",
    "Curse of Dimensionality in Distance Metrics:\n",
    "\n",
    "Traditional distance metrics (e.g., Euclidean distance) become less meaningful in high-dimensional spaces.\n",
    "Points tend to be roughly equidistant from each other, making it challenging for distance-based algorithms to discriminate between them.\n",
    "Overfitting:\n",
    "\n",
    "High-dimensional models are more susceptible to overfitting, as they can memorize noise in the training data rather than capturing meaningful patterns.\n",
    "Regularization techniques become crucial to mitigate overfitting.\n",
    "Increased Model Complexity:\n",
    "\n",
    "High-dimensional models often have more parameters, leading to increased model complexity.\n",
    "More complex models may require more data to avoid overfitting.\n",
    "Importance in Machine Learning:\n",
    "Feature Selection and Dimensionality Reduction Techniques:\n",
    "\n",
    "Addressing the curse of dimensionality is crucial for effective feature selection and dimensionality reduction techniques.\n",
    "Principal Component Analysis (PCA), t-distributed Stochastic Neighbor Embedding (t-SNE), and other methods help reduce the dimensionality while preserving essential information.\n",
    "Improved Model Generalization:\n",
    "\n",
    "Reducing dimensionality helps in creating models that generalize better to new, unseen data.\n",
    "It mitigates overfitting by focusing on the most informative features and capturing essential patterns.\n",
    "Computational Efficiency:\n",
    "\n",
    "Lower-dimensional representations simplify computations, making algorithms more computationally efficient.\n",
    "Dimensionality reduction techniques enable faster training and inference.\n",
    "Interpretability:\n",
    "\n",
    "Lower-dimensional representations often lead to more interpretable models that are easier to understand and analyze.\n",
    "Addressing the curse of dimensionality is a critical step in building accurate, efficient, and interpretable machine learning models, especially when dealing with datasets containing a large number of features. Techniques that aim to reduce dimensionality play a central role in overcoming these challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e2e625-aef1-42c2-b017-6656bce8bb21",
   "metadata": {},
   "source": [
    " Q2. How does the curse of dimensionality impact the performance of machine learning algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf27cbb4-4ab8-4f77-8dd5-abbb0d6257f6",
   "metadata": {},
   "source": [
    "\n",
    "The curse of dimensionality can significantly impact the performance of machine learning algorithms in various ways. As the number of dimensions (features) increases, several challenges and phenomena arise, affecting the efficiency and effectiveness of algorithms. Here are some ways in which the curse of dimensionality impacts machine learning algorithms:\n",
    "\n",
    "Increased Data Sparsity:\n",
    "\n",
    "In high-dimensional spaces, the available data becomes sparser as the number of dimensions increases.\n",
    "Sparse data can lead to overfitting, as models might memorize noise in the training data instead of learning meaningful patterns.\n",
    "Computational Complexity:\n",
    "\n",
    "High-dimensional datasets require more computational resources and time for training and inference.\n",
    "Algorithms that rely on distance computations, such as K-nearest neighbors (KNN), become less efficient due to increased distances between points.\n",
    "Difficulty in Visualization:\n",
    "\n",
    "As the number of dimensions grows, it becomes increasingly challenging to visualize the data.\n",
    "Understanding the relationships and patterns within the data becomes difficult, hindering the interpretability of models.\n",
    "Degraded Performance of Distance Metrics:\n",
    "\n",
    "Traditional distance metrics (e.g., Euclidean distance) become less meaningful in high-dimensional spaces.\n",
    "Points tend to be roughly equidistant from each other, making it difficult for distance-based algorithms to discriminate between them.\n",
    "Overfitting:\n",
    "\n",
    "High-dimensional models are more susceptible to overfitting, as they can memorize noise in the training data rather than capturing meaningful patterns.\n",
    "More complex models with many parameters may overfit the training data.\n",
    "Increased Model Complexity:\n",
    "\n",
    "High-dimensional models often have more parameters, leading to increased model complexity.\n",
    "Increased complexity can result in models that are harder to interpret and prone to overfitting.\n",
    "Reduced Generalization Performance:\n",
    "\n",
    "High-dimensional models may struggle to generalize well to new, unseen data, especially when the training data is limited.\n",
    "The risk of overfitting increases as the dimensionality grows.\n",
    "Diminished Feature Importance:\n",
    "\n",
    "In high-dimensional spaces, it becomes challenging to identify which features are truly important for the task at hand.\n",
    "Some features may have little impact, and distinguishing between relevant and irrelevant features becomes more difficult.\n",
    "Mitigating the Impact:\n",
    "Feature Selection and Dimensionality Reduction:\n",
    "\n",
    "Techniques such as feature selection and dimensionality reduction (e.g., PCA) can help mitigate the impact by focusing on the most informative features.\n",
    "Regularization:\n",
    "\n",
    "Regularization techniques can be employed to prevent overfitting by penalizing complex models with many parameters.\n",
    "Ensemble Methods:\n",
    "\n",
    "Ensemble methods like random forests and gradient boosting can provide robustness to high-dimensional data by combining multiple weak models.\n",
    "Domain Knowledge:\n",
    "\n",
    "Leveraging domain knowledge to guide feature selection and model building can help create more effective models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7b527e-d656-46f7-b6a1-353953c774e8",
   "metadata": {},
   "source": [
    "Q3. What are some of the consequences of the curse of dimensionality in machine learning, and how do\n",
    "they impact model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35bba22-5c3a-4b1c-aa03-0e10e6688417",
   "metadata": {},
   "source": [
    "\n",
    "The curse of dimensionality in machine learning has several consequences, and its impact on model performance can be significant. Here are some of the consequences and how they affect the performance of machine learning models:\n",
    "\n",
    "Increased Data Sparsity:\n",
    "\n",
    "Consequence: As the number of dimensions increases, the volume of the data space grows exponentially. Consequently, the available data becomes sparser.\n",
    "Impact: Sparse data can lead to overfitting, where models may memorize noise in the training data rather than learning meaningful patterns. Generalization to new, unseen data becomes challenging.\n",
    "Computational Complexity:\n",
    "\n",
    "Consequence: High-dimensional datasets require more computational resources and time for training and inference.\n",
    "Impact: Increased computational complexity can lead to slower training and inference times. Algorithms that rely on distance calculations, such as K-nearest neighbors, become less efficient due to the larger number of pairwise distances to compute.\n",
    "Difficulty in Visualization:\n",
    "\n",
    "Consequence: Visualizing data in high-dimensional spaces becomes challenging or impossible for humans.\n",
    "Impact: Understanding relationships and patterns within the data becomes difficult. Interpretability and intuition about the data may suffer, hindering the ability to make informed decisions based on model insights.\n",
    "Degraded Performance of Distance Metrics:\n",
    "\n",
    "Consequence: Traditional distance metrics (e.g., Euclidean distance) become less meaningful in high-dimensional spaces.\n",
    "Impact: Distance-based algorithms may struggle to distinguish between points, as distances tend to become more uniform. This can impact the performance of clustering and classification algorithms that rely on proximity.\n",
    "Increased Risk of Overfitting:\n",
    "\n",
    "Consequence: High-dimensional models are more prone to overfitting, where the model captures noise in the training data rather than the underlying patterns.\n",
    "Impact: Models may perform well on the training data but fail to generalize to new data. Regularization and feature selection become crucial to prevent overfitting.\n",
    "Increased Model Complexity:\n",
    "\n",
    "Consequence: High-dimensional models often have more parameters, leading to increased model complexity.\n",
    "Impact: More complex models may require larger amounts of data to avoid overfitting. Interpreting and understanding such models become challenging.\n",
    "Reduced Generalization Performance:\n",
    "\n",
    "Consequence: High-dimensional models may struggle to generalize well to new, unseen data, especially when the training data is limited.\n",
    "Impact: The risk of overfitting increases, and models may fail to capture the true underlying structure of the data.\n",
    "Diminished Feature Importance:\n",
    "\n",
    "Consequence: In high-dimensional spaces, distinguishing between relevant and irrelevant features becomes more challenging.\n",
    "Impact: Some features may have little impact on the target variable, making it difficult to identify and focus on the most informative features. This can lead to suboptimal model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddabc90-7ae9-4ab7-a4aa-63e6cf0b29d6",
   "metadata": {},
   "source": [
    "Q4. Can you explain the concept of feature selection and how it can help with dimensionality reduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772d357e-15d7-4d5c-ac43-bb624319daf7",
   "metadata": {},
   "source": [
    "Certainly! Feature selection is a process in machine learning where a subset of the most relevant features (input variables or attributes) is chosen from the original set. The goal is to retain the most informative features while eliminating irrelevant or redundant ones. Feature selection can help with dimensionality reduction, addressing the challenges posed by the curse of dimensionality. Here's an explanation of the concept and its benefits:\n",
    "\n",
    "Concept of Feature Selection:\n",
    "Relevance of Features:\n",
    "\n",
    "Not all features in a dataset contribute equally to the predictive power of a model. Some features may be redundant, irrelevant, or even noisy.\n",
    "Curse of Dimensionality:\n",
    "\n",
    "As the number of features increases, the volume of the data space grows exponentially, leading to challenges such as increased computational complexity, sparsity of data, and overfitting.\n",
    "Objectives of Feature Selection:\n",
    "\n",
    "Improve model performance by focusing on the most informative features.\n",
    "Reduce the risk of overfitting by eliminating irrelevant or noisy features.\n",
    "Enhance model interpretability and understanding.\n",
    "Techniques for Feature Selection:\n",
    "Filter Methods:\n",
    "\n",
    "Evaluate the relevance of features based on statistical measures or correlation with the target variable.\n",
    "Examples include Information Gain, Chi-squared test, and correlation coefficients.\n",
    "Wrapper Methods:\n",
    "\n",
    "Use a specific machine learning model to evaluate subsets of features based on their impact on model performance.\n",
    "Examples include Recursive Feature Elimination (RFE) and Forward/Backward Selection.\n",
    "Embedded Methods:\n",
    "\n",
    "Incorporate feature selection as part of the model training process.\n",
    "Examples include LASSO (L1 regularization) and decision tree-based algorithms like Random Forests.\n",
    "Benefits of Feature Selection for Dimensionality Reduction:\n",
    "Improved Model Performance:\n",
    "\n",
    "By focusing on relevant features, models can achieve better generalization performance on new, unseen data.\n",
    "Reducing the number of irrelevant or redundant features can help prevent overfitting.\n",
    "Computational Efficiency:\n",
    "\n",
    "Training and inference times are reduced when working with a smaller set of features.\n",
    "Complexity is decreased, making algorithms more computationally efficient.\n",
    "Enhanced Interpretability:\n",
    "\n",
    "Models with fewer features are often more interpretable and easier to understand.\n",
    "Clear identification of important features facilitates better model interpretation.\n",
    "Addressing the Curse of Dimensionality:\n",
    "\n",
    "Feature selection directly mitigates the challenges posed by the curse of dimensionality by selecting the most informative features and discarding less relevant ones.\n",
    "Considerations:\n",
    "Domain Knowledge:\n",
    "\n",
    "Incorporating domain knowledge is valuable in guiding feature selection. Domain experts can provide insights into the relevance of specific features.\n",
    "Trade-off with Model Complexity:\n",
    "\n",
    "Striking a balance between reducing dimensionality and maintaining model complexity is essential. Extremely aggressive feature reduction may lead to loss of important information.\n",
    "Validation and Evaluation:\n",
    "\n",
    "Feature selection should be performed while considering the impact on model performance. Validation on a separate dataset is crucial to ensure generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0176a507-a92a-462d-a1fe-20f71d007f68",
   "metadata": {},
   "source": [
    "Q5. What are some limitations and drawbacks of using dimensionality reduction techniques in machine\n",
    "learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7a74b3-5ba7-497b-bff4-89568f993dc4",
   "metadata": {},
   "source": [
    "\n",
    "While dimensionality reduction techniques offer valuable benefits, they also come with certain limitations and drawbacks that should be considered. Here are some common limitations associated with using dimensionality reduction techniques in machine learning:\n",
    "\n",
    "Loss of Information:\n",
    "\n",
    "One of the primary concerns is the potential loss of information during dimensionality reduction. By reducing the number of features, some relevant information may be discarded, leading to a less accurate representation of the data.\n",
    "Difficulty in Interpretability:\n",
    "\n",
    "Reduced-dimensional representations may be challenging to interpret, especially when the original features have complex interactions. Understanding the meaning of the transformed features can be non-trivial.\n",
    "Sensitivity to Outliers:\n",
    "\n",
    "Dimensionality reduction methods can be sensitive to outliers in the data. Outliers may have a strong influence on the outcome of certain techniques, affecting the quality of the reduced-dimensional representation.\n",
    "Algorithm Dependence:\n",
    "\n",
    "The effectiveness of dimensionality reduction techniques is often dependent on the choice of algorithm and its parameters. Different algorithms may yield different results, and the optimal choice may vary based on the characteristics of the data.\n",
    "Nonlinear Relationships:\n",
    "\n",
    "Linear dimensionality reduction techniques, such as PCA, assume linear relationships between features. In cases where the relationships are nonlinear, linear methods may not capture the underlying structure effectively. Nonlinear methods like t-Distributed Stochastic Neighbor Embedding (t-SNE) can be more suitable but are computationally expensive.\n",
    "Computational Complexity:\n",
    "\n",
    "Some dimensionality reduction techniques, especially nonlinear ones, can be computationally expensive and may require significant resources. This can be a limitation when dealing with large datasets.\n",
    "Noisy Data Handling:\n",
    "\n",
    "Noise in the data can impact the performance of dimensionality reduction. Techniques like PCA are sensitive to noise, and the reduced-dimensional representation may be influenced by noise in the original data.\n",
    "Loss of Separability:\n",
    "\n",
    "In certain cases, dimensionality reduction may lead to reduced separability between classes in a classification task. This can adversely affect the performance of classifiers trained on the reduced-dimensional data.\n",
    "Curse of Dimensionality Trade-Off:\n",
    "\n",
    "While dimensionality reduction can help address the curse of dimensionality, it introduces a trade-off. The reduction in dimensionality may be beneficial for some algorithms but detrimental for others, depending on the nature of the problem.\n",
    "Parameter Sensitivity:\n",
    "\n",
    "Some dimensionality reduction techniques have parameters that need to be tuned. The performance of these methods can be sensitive to the choice of parameters, and finding the optimal settings may require experimentation.\n",
    "Assumption of Linearity:\n",
    "\n",
    "Linear dimensionality reduction methods assume that the relationship between features is linear. If the underlying relationships are highly nonlinear, linear methods may not capture the essential structure of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2fce56-356e-41d9-bf19-06a0da2c8a45",
   "metadata": {},
   "source": [
    "Q6. How does the curse of dimensionality relate to overfitting and underfitting in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd7da2a-737f-4c39-b2dc-8d5bd14e6afb",
   "metadata": {},
   "source": [
    "The curse of dimensionality is closely related to overfitting and underfitting in machine learning, and understanding this relationship is crucial for building effective models. Here's how the curse of dimensionality is connected to overfitting and underfitting:\n",
    "\n",
    "1. Overfitting:\n",
    "Curse of Dimensionality Connection:\n",
    "\n",
    "In high-dimensional spaces, the volume of the data space grows exponentially with the number of dimensions. As a result, the available data becomes sparser, and data points may be farther apart from each other.\n",
    "In a high-dimensional space, a model can find spurious patterns or memorize noise in the training data because there are more opportunities to fit the noise.\n",
    "Impact on Overfitting:\n",
    "\n",
    "The increased sparsity and the potential for capturing noise in the data make models more prone to overfitting. A model that overfits the training data performs well on the training set but fails to generalize to new, unseen data.\n",
    "Mitigation:\n",
    "\n",
    "Techniques to address overfitting in high-dimensional spaces include regularization methods, feature selection, and dimensionality reduction. These approaches help prevent the model from fitting noise and encourage the learning of more meaningful patterns.\n",
    "2. Underfitting:\n",
    "Curse of Dimensionality Connection:\n",
    "\n",
    "Underfitting occurs when a model is too simple to capture the underlying patterns in the data. In high-dimensional spaces, building an accurate model becomes challenging due to the increased complexity and sparsity.\n",
    "Impact on Underfitting:\n",
    "\n",
    "The curse of dimensionality can exacerbate underfitting because a simple model may struggle to represent the relationships between features in a high-dimensional space. The model may fail to capture the true structure of the data.\n",
    "Mitigation:\n",
    "\n",
    "Increasing model complexity, choosing more expressive models, and carefully selecting relevant features can help mitigate underfitting. However, finding the right balance is essential to avoid overfitting.\n",
    "Summary:\n",
    "The curse of dimensionality introduces challenges related to sparsity, increased computational complexity, and the potential for overfitting.\n",
    "Overfitting is more likely in high-dimensional spaces due to the increased risk of capturing noise in the data.\n",
    "Underfitting can be exacerbated in high-dimensional spaces as simple models may struggle to represent the complexities present in the data.\n",
    "Techniques such as regularization, feature selection, and dimensionality reduction play a key role in mitigating the impact of the curse of dimensionality and addressing overfitting and underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3599074b-6150-48cd-8239-525f86e06caa",
   "metadata": {},
   "source": [
    "Q7. How can one determine the optimal number of dimensions to reduce data to when using\n",
    "dimensionality reduction techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c911602-4c92-4d72-8a4a-0a7458799146",
   "metadata": {},
   "source": [
    "Determining the optimal number of dimensions (features) to reduce data to when using dimensionality reduction techniques is a crucial aspect of the process. The choice of the number of dimensions impacts the performance, interpretability, and computational efficiency of the model. Here are some approaches to help determine the optimal number of dimensions:\n",
    "\n",
    "1. Explained Variance:\n",
    "For techniques like Principal Component Analysis (PCA), the explained variance indicates the proportion of the dataset's total variance that is retained by each principal component.\n",
    "Plot the cumulative explained variance against the number of dimensions. Choose the number of dimensions where the cumulative explained variance is high enough to capture most of the dataset's variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb6176d-ae95-49bc-91ee-a933d1a7d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Fit PCA\n",
    "pca = PCA().fit(X)\n",
    "\n",
    "# Plot explained variance ratio\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Dimensions')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33108778-04a5-4010-8f84-4d45d5aa2fb6",
   "metadata": {},
   "source": [
    "2. Cross-Validation:\n",
    "Use cross-validation to evaluate the model's performance for different numbers of dimensions. Choose the number of dimensions that results in the best cross-validated performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797af5a5-324a-4950-8877-c7271bdacf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a pipeline with dimensionality reduction and a model\n",
    "pipeline = Pipeline([\n",
    "    ('reduce_dim', PCA(n_components=n)),\n",
    "    ('classify', YourModel())  # Replace YourModel() with the actual model\n",
    "])\n",
    "\n",
    "# Perform cross-validation for different numbers of dimensions\n",
    "scores = []\n",
    "for n in range(1, max_dimensions + 1):\n",
    "    pipeline.set_params(reduce_dim__n_components=n)\n",
    "    cv_score = np.mean(cross_val_score(pipeline, X, y, cv=5))\n",
    "    scores.append(cv_score)\n",
    "\n",
    "# Choose the number of dimensions with the highest cross-validated score\n",
    "optimal_dimensions = np.argmax(scores) + 1\n",
    "print(f\"Optimal number of dimensions: {optimal_dimensions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ab72a6-7f7f-4bdb-8816-a1cce5a11b8f",
   "metadata": {},
   "source": [
    "3. Elbow Method:\n",
    "For methods like k-means clustering, you can use the \"elbow method.\" Plot the cost (inertia) of the clustering algorithm against the number of dimensions and choose the point where the rate of decrease slows down (the \"elbow\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecded5f4-16ff-417f-b95c-02bb8af93a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Fit k-means clustering for different numbers of dimensions\n",
    "inertias = []\n",
    "for n in range(1, max_dimensions + 1):\n",
    "    kmeans = KMeans(n_clusters=n, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.plot(range(1, max_dimensions + 1), inertias, marker='o')\n",
    "plt.xlabel('Number of Dimensions (Clusters)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87be8653-a169-41ac-b16e-fd121e7b3d8b",
   "metadata": {},
   "source": [
    "4. Model Performance:\n",
    "Evaluate the model's performance on a validation set or through cross-validation for different numbers of dimensions. Choose the number of dimensions that maximizes performance metrics like accuracy, F1 score, or other relevant measures.\n",
    "5. Domain Knowledge:\n",
    "Consider domain knowledge and the requirements of the specific task. Some applications may have constraints or preferences that guide the choice of the number of dimensions.\n",
    "6. Visualization:\n",
    "If possible, visualize the data in reduced dimensions and inspect the results. Choose a number of dimensions that provides a good balance between capturing important patterns and reducing complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c557326-4a2d-4520-a678-2227e7f8d4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
