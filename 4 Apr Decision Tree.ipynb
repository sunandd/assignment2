{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b6ce13f-8c1f-460e-b8ce-63919eaf72a7",
   "metadata": {},
   "source": [
    "Q1. Describe the decision tree classifier algorithm and how it works to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f6eaba-bbaf-4a0d-acdf-6f28bf4caadf",
   "metadata": {},
   "source": [
    "classification and regression tasks. It is a supervised learning algorithm that works by recursively partitioning the feature space into subsets and assigning a class label to each subset.\n",
    "\n",
    "Here's how the Decision Tree Classifier algorithm works:\n",
    "\n",
    "Tree Construction:\n",
    "\n",
    "The algorithm starts by selecting the best feature from the dataset to split the data into subsets. The \"best\" feature is determined using criteria like Gini impurity, information gain, or gain ratio. These criteria measure the homogeneity of the classes in each subset after the split.\n",
    "The selected feature is used as the root node of the tree, and the dataset is split into multiple subsets based on the possible values of that feature.\n",
    "Recursive Splitting:\n",
    "\n",
    "For each subset created in the previous step, the algorithm repeats the process of selecting the best feature to split that subset further. This process is carried out recursively until a stopping condition is met. The stopping condition could be a maximum tree depth, a minimum number of samples required to split a node, or when the data in a node belongs to a single class.\n",
    "Assigning Labels:\n",
    "\n",
    "As the tree grows, each internal node represents a decision based on a feature, and each leaf node represents a predicted class label.\n",
    "When a new data point is introduced for prediction, it traverses down the tree from the root node to a leaf node. At each internal node, the decision is made based on the value of the corresponding feature in the data point. This process continues until the data point reaches a leaf node.\n",
    "Class Label Assignment:\n",
    "\n",
    "Once the data point reaches a leaf node, the majority class label of the training samples in that leaf node is assigned as the predicted class label for the new data point. In the case of a tie, the algorithm might use additional rules to break the tie, such as selecting the class that occurs first in the class label list.\n",
    "Prediction:\n",
    "\n",
    "After traversal, the final assigned class label is the prediction made by the Decision Tree Classifier for the given data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3203bd0b-a965-42d8-8ce4-ef03eb5ff18b",
   "metadata": {},
   "source": [
    "Q2. Provide a step-by-step explanation of the mathematical intuition behind decision tree classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1251616-5888-4de5-ba23-9c9f966b7723",
   "metadata": {},
   "source": [
    "Step 1: Choosing the Splitting Criterion\n",
    "The first step in building a Decision Tree involves selecting the best feature to split the data. This is done by evaluating different splitting criteria. One common criterion is the Gini impurity, which measures the degree of impurity or disorder in a set of data points. Mathematically, the Gini impurity of a set S is calculated as:\n",
    "\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "1\n",
    "−\n",
    "∑\n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    ")\n",
    "2\n",
    "Gini(S)=1−∑ \n",
    "i\n",
    "​\n",
    " (p \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "i iterates over the classes in the dataset.\n",
    "�\n",
    "�\n",
    "p \n",
    "i\n",
    "​\n",
    "  is the proportion of data points in class \n",
    "�\n",
    "i within set \n",
    "�\n",
    "S.\n",
    "Step 2: Finding the Best Split\n",
    "For each feature, the algorithm calculates the Gini impurity of the subsets formed by splitting the data based on different values of that feature. The feature and value that result in the lowest Gini impurity after the split are chosen as the splitting criteria.\n",
    "\n",
    "Step 3: Recursively Splitting\n",
    "Once the best feature and value are chosen, the dataset is split into subsets based on this criterion. This process is repeated for each subset, creating a tree-like structure. At each step, the Gini impurity is minimized by selecting the best feature and value for splitting.\n",
    "\n",
    "Step 4: Assigning Class Labels\n",
    "As the tree grows, the leaves represent subsets of the data. The class label assigned to a leaf is usually determined by majority voting. The most common class within the leaf becomes the predicted class for instances that end up in that leaf.\n",
    "\n",
    "Step 5: Making Predictions\n",
    "When a new data point is introduced for prediction, it traverses down the tree from the root to a leaf. At each internal node, the algorithm checks the corresponding feature value of the data point and decides whether to go left or right based on the splitting value. This process continues until the data point reaches a leaf node. The predicted class for the data point is then the majority class within that leaf.\n",
    "\n",
    "Step 6: Handling Overfitting\n",
    "Decision Trees are prone to overfitting, meaning they can become overly complex and fit noise in the training data. To prevent this, techniques like pruning are applied. Pruning involves removing branches from the tree that do not significantly improve its performance on validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599ec268-0ec4-4838-ad5b-000e5b565040",
   "metadata": {},
   "source": [
    "Q3. Explain how a decision tree classifier can be used to solve a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919f3d39-e75e-406b-82b1-b438e98985d1",
   "metadata": {},
   "source": [
    "Step 1: Data Preparation\n",
    "You start with a dataset containing labeled examples. Each example consists of a set of features and a binary class label (usually 0 or 1) indicating the category the example belongs to.\n",
    "\n",
    "Step 2: Building the Decision Tree\n",
    "\n",
    "Selecting the Splitting Criterion: The first step is to choose a splitting criterion, which helps determine how to divide the data at each node of the tree. Common criteria include Gini impurity, information gain, and gain ratio.\n",
    "\n",
    "Choosing the Root Node: The algorithm selects the best feature and corresponding threshold (or value) to split the data. The \"best\" feature is chosen based on the chosen criterion. It's the feature that leads to the most significant separation between the classes.\n",
    "\n",
    "Splitting the Data: The dataset is divided into two subsets based on the chosen feature and threshold. One subset contains instances with feature values less than or equal to the threshold, while the other subset contains instances with feature values greater than the threshold.\n",
    "\n",
    "Repeating the Process: The algorithm recursively repeats the splitting process for each subset created in the previous step. It chooses the best feature and threshold again for each subset and continues until a stopping condition is met, such as reaching a maximum tree depth or having a minimum number of samples in a node.\n",
    "\n",
    "Step 3: Assigning Class Labels\n",
    "\n",
    "Majority Voting: Once the Decision Tree is constructed, each leaf node represents a prediction. The class label assigned to a leaf node is determined by majority voting. In other words, the class that appears most frequently in the instances within that leaf node is chosen as the predicted class.\n",
    "Step 4: Making Predictions\n",
    "\n",
    "Traversing the Tree: To make a prediction for a new instance, you start at the root node of the Decision Tree. You compare the value of the relevant feature for the instance with the threshold at the root node. Depending on whether the value is less than or equal to the threshold, you move to the left or right child node.\n",
    "\n",
    "Repeating the Process: You continue traversing down the tree by comparing the feature values with thresholds at each node until you reach a leaf node. The class label associated with the leaf node becomes the prediction for the instance.\n",
    "\n",
    "Step 5: Handling Overfitting\n",
    "\n",
    "Pruning: Decision Trees can become too complex and overfit the training data. Pruning involves removing nodes that don't contribute much to the overall accuracy of the tree. This helps improve the tree's generalization ability to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a2c800-a2a8-4a5d-945a-bbf3c382a1fa",
   "metadata": {},
   "source": [
    "Q4. Discuss the geometric intuition behind decision tree classification and how it can be used to make\n",
    "predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612f490c-ff91-419d-821d-057276fa57ca",
   "metadata": {},
   "source": [
    "\n",
    "Geometric intuition behind Decision Tree classification:\n",
    "\n",
    "The Decision Tree classification algorithm can be understood geometrically by visualizing the process of recursively partitioning the feature space. Imagine a scatter plot where each data point represents an instance with its features plotted on different axes. The goal of the Decision Tree is to draw boundaries that separate different classes as cleanly as possible.\n",
    "\n",
    "Feature Axes as Dimensions: In a binary classification problem with two features, you can think of the axes as representing the dimensions of the feature space. Each data point is a point in this space, and the classes are divided into regions or \"boxes.\"\n",
    "\n",
    "Splitting the Space: At each node of the Decision Tree, the algorithm selects a feature and a threshold to split the data. This corresponds to drawing a boundary along one of the feature axes. For instance, if the chosen feature is along the x-axis and the threshold is 5, the data is split into two regions: points with x-values less than or equal to 5 and points with x-values greater than 5.\n",
    "\n",
    "Recursive Partitioning: As the tree grows, the feature space is repeatedly divided into smaller and smaller regions. These regions correspond to the leaf nodes of the tree. Each region is associated with a class label, typically determined by majority voting.\n",
    "\n",
    "Decision Boundaries: The boundaries created by Decision Trees are aligned with the axes, and they are orthogonal (perpendicular) to the feature axes. This is because each decision in the tree is based on a single feature and its threshold. Decision boundaries are drawn such that they minimize impurity (Gini impurity, for example) or maximize information gain.\n",
    "\n",
    "Using the Geometric Intuition for Predictions:\n",
    "\n",
    "When it comes to making predictions using a trained Decision Tree, the geometric intuition becomes apparent:\n",
    "\n",
    "Traversing the Tree: To predict the class of a new data point, you start at the root of the tree and compare the feature value at that node with the threshold. Depending on the comparison, you move to the left or right child node, which corresponds to navigating through the feature space based on the selected feature and threshold.\n",
    "\n",
    "Arriving at a Leaf Node: You continue traversing the tree by comparing feature values and moving through the feature space until you reach a leaf node. The leaf node's associated class label becomes the predicted class for the new data point.\n",
    "\n",
    "Decision Boundaries for Predictions: The path you take through the Decision Tree from the root to a leaf node represents the decision boundaries that determine the predicted class. These decision boundaries are axis-aligned and are based on the splits made during the tree's construction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8658d0d3-1315-45d9-b439-737fe6ddd381",
   "metadata": {},
   "source": [
    "Q5. Define the confusion matrix and describe how it can be used to evaluate the performance of a\n",
    "classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3d54ec-dfb2-4d00-a61b-663dc2b63e61",
   "metadata": {},
   "source": [
    "The confusion matrix is a performance evaluation tool used in classification to assess the performance of a machine learning model by breaking down its predictions into various categories. It provides a clear summary of how well the model's predictions align with the actual ground truth labels.\n",
    "\n",
    "True Positive (TP): The model correctly predicted the positive class when the actual class was indeed positive.\n",
    "\n",
    "False Positive (FP): The model incorrectly predicted the positive class when the actual class was negative. Also known as a Type I error or a \"false alarm.\"\n",
    "\n",
    "False Negative (FN): The model incorrectly predicted the negative class when the actual class was positive. Also known as a Type II error or a \"miss.\"\n",
    "\n",
    "True Negative (TN): The model correctly predicted the negative class when the actual class was indeed negative.\n",
    "\n",
    "Using the Confusion Matrix to Evaluate Model Performance:\n",
    "\n",
    "Accuracy: This is the most common metric and measures the overall correctness of the model's predictions. It is calculated as:\n",
    "accuracy=TP+TN/TP+TN+FP+FN\n",
    "\n",
    "Precision: Precision is the proportion of correctly predicted positive instances out of all instances predicted as positive. It focuses on the accuracy of positive predictions.\n",
    "PRECISION=TP/TP+FP\n",
    "\n",
    "Recall (Sensitivity or True Positive Rate): Recall measures the proportion of actual positive instances that were correctly predicted by the model. It's a measure of how well the model captures positive instances.\n",
    "RECALL=TP/TP+FN\n",
    "\n",
    "Specificity (True Negative Rate): Specificity measures the proportion of actual negative instances that were correctly predicted by the model.\n",
    "SPECIFICITY=TN/TN+FP\n",
    "\n",
    "F1-Score: The F1-score is the harmonic mean of precision and recall. It balances the trade-off between precision and recall and is particularly useful when classes are imbalanced.\n",
    "F1-SCORE= 2*PRECISION*RECALL/PRECISION+RECALL\n",
    "\n",
    "ROC Curve and AUC: The Receiver Operating Characteristic (ROC) curve is a graphical representation of the model's trade-off between sensitivity (recall) and specificity. The Area Under the Curve (AUC) summarizes the ROC curve's performance in a single value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa19624-bb4f-4045-a44f-ce30323e7f88",
   "metadata": {},
   "source": [
    "Q6. Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be\n",
    "calculated from it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdd0956-bbdc-41b4-906e-a8f5595c7bba",
   "metadata": {},
   "source": [
    "True Positive (TP): 120 emails were correctly predicted as spam.\n",
    "False Positive (FP): 20 emails that were actually not spam were incorrectly predicted as spam.\n",
    "False Negative (FN): 30 emails that were actually spam were incorrectly predicted as not spam.\n",
    "True Negative (TN): 130 emails were correctly predicted as not spam.\n",
    "Now, let's calculate precision, recall, and F1-score using these values:\n",
    "\n",
    "Precision:\n",
    "Precision measures how many of the predicted spam emails are actually spam.\n",
    "\n",
    "Precision\n",
    "\n",
    "120+20\n",
    "120\n",
    "​\n",
    " =0.857\n",
    "\n",
    "Precision tells us that out of all the emails predicted as spam, 85.7% are truly spam.\n",
    "\n",
    "Recall (Sensitivity):\n",
    "Recall measures how many of the actual spam emails were correctly predicted.\n",
    "\n",
    "Recall\n",
    "=\n",
    "\n",
    " = \n",
    "120+30\n",
    "120\n",
    "​\n",
    " =0.8\n",
    "\n",
    "Recall indicates that the model captured 80% of the actual spam emails.\n",
    "\n",
    "F1-Score:\n",
    "The F1-score balances precision and recall by taking their harmonic mean. It provides a single metric that considers both false positives and false negatives.\n",
    "\n",
    "F1-Score\n",
    "=\n",
    "2\n",
    "×\n",
    "Precision\n",
    "×\n",
    "Recall\n",
    "Precision\n",
    "+\n",
    "Recall\n",
    "=\n",
    "2\n",
    "×\n",
    "0.857\n",
    "×\n",
    "0.8\n",
    "0.857\n",
    "+\n",
    "0.8\n",
    "≈\n",
    "0.827\n",
    "F1-Score= \n",
    "Precision+Recall\n",
    "2×Precision×Recall\n",
    "​\n",
    " = \n",
    "0.857+0.8\n",
    "2×0.857×0.8\n",
    "​\n",
    " ≈0.827\n",
    "\n",
    "The F1-score is 0.827, which is a combined measure of precision and recall. It's useful when you want to balance the trade-off between minimizing false positives and false negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc9019-b6e4-48ee-b7b2-37f6d86af9b2",
   "metadata": {},
   "source": [
    "Q6. Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be\n",
    "calculated from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ad1c00-4cf5-4ba9-8d7d-4be94d0edb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "                  Actual Positive      Actual Negative\n",
    "Predicted Positive      100 (True Positive)    20 (False Positive)\n",
    "Predicted Negative      10 (False Negative)    500 (True Negative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f46f40-9cbf-4cea-8e90-8c0daa7a375d",
   "metadata": {},
   "source": [
    "True Positive (TP): The model correctly predicted instances as Positive when they were actually Positive .\n",
    "\n",
    "False Positive (FP): The model incorrectly predicted instances as Positive when they were actually Negative.\n",
    "\n",
    "False Negative (FN): The model incorrectly predicted instances as Negative when they were actually Positive.\n",
    "\n",
    "True Negative (TN): The model correctly predicted instances as Negative when they were actually Negative ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fb2bcd-27fa-49aa-9bee-0331d5bf084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precision:\n",
    "Precision measures the accuracy of the positive predictions made by the model.\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "Precision = 100 / (100 + 20)\n",
    "Precision = 0.8333 (rounded to 4 decimal places)\n",
    "\n",
    "Recall (Sensitivity):\n",
    "Recall measures the ability of the model to identify all the actual positive instances.\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "Recall = 100 / (100 + 10)\n",
    "Recall = 0.9091 (rounded to 4 decimal places)\n",
    "\n",
    "F1 Score:\n",
    "The F1 Score is the harmonic mean of Precision and Recall. It provides a balance between Precision and Recall.\n",
    "\n",
    "F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "F1 Score = 2 * (0.8333 * 0.9091) / (0.8333 + 0.9091)\n",
    "F1 Score = 0.8696 (rounded to 4 decimal places)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27daba2-a3d7-40f1-8257-8d99c51b5113",
   "metadata": {},
   "source": [
    "Q7. Discuss the importance of choosing an appropriate evaluation metric for a classification problem and\n",
    "explain how this can be done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb0488d-490d-4ab5-bbd8-24b12dc418be",
   "metadata": {},
   "source": [
    "Choosing an appropriate evaluation metric for a classification problem is crucial because it determines how you assess the performance of your model and whether it aligns with your specific objectives and requirements. The choice of metric should reflect the nature of the problem and the relative importance of different types of errors (e.g., false positives and false negatives). Here are some important points to consider when selecting an evaluation metric:\n",
    "\n",
    "Nature of the Problem:\n",
    "\n",
    "Binary Classification: If you have a binary classification problem (two classes), you might consider metrics like accuracy, precision, recall, F1 score, or the area under the ROC curve (AUC-ROC).\n",
    "Multiclass Classification: For problems with more than two classes, you may need to consider metrics like categorical accuracy, confusion matrix, or class-specific metrics (precision, recall, F1 score) for each class.\n",
    "Imbalanced Data:\n",
    "\n",
    "In cases where one class significantly outnumbers the other, accuracy may not be an appropriate metric. Models can achieve high accuracy by simply predicting the majority class, ignoring the minority class. In such cases, metrics like precision, recall, or the F1 score are often more informative as they focus on the performance of the minority class.\n",
    "Business Objectives:\n",
    "\n",
    "The choice of metric should align with the goals of your project or business. Consider the cost or consequences associated with different types of errors (false positives vs. false negatives). For example, in a medical diagnosis scenario, missing a disease (false negative) might have severe consequences, so recall might be more important than precision.\n",
    "Threshold Selection:\n",
    "\n",
    "Many classification algorithms provide probability scores as outputs. You can adjust the decision threshold to trade off between precision and recall. This can be particularly important when you need to balance the trade-off between false positives and false negatives.\n",
    "Domain Knowledge:\n",
    "\n",
    "Your knowledge of the domain and the specific problem can guide metric selection. Some domains may have established best practices for evaluation metrics. Consulting with domain experts can be invaluable.\n",
    "Model Complexity:\n",
    "\n",
    "The choice of metric can also be influenced by the complexity of your model. More complex models might require more robust evaluation metrics to understand their performance thoroughly.\n",
    "Visualizing Results:\n",
    "\n",
    "Sometimes, it's helpful to visualize the performance of your model using tools like ROC curves, precision-recall curves, or confusion matrices. These visualizations can provide a more comprehensive understanding of model behavior.\n",
    "Cross-Validation:\n",
    "\n",
    "Use techniques like k-fold cross-validation to ensure that your choice of metric is stable across different subsets of your data. This helps in reducing the impact of data variability on metric values.\n",
    "Comparative Analysis:\n",
    "\n",
    "If you are comparing multiple models, it's essential to use the same evaluation metric for all models to make a fair comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2743816a-dc9c-4fdf-ae0a-95f773627ec7",
   "metadata": {},
   "source": [
    "Q8. Provide an example of a classification problem where precision is the most important metric, and\n",
    "explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fc18e5-ffe1-43d3-aa5b-fb1dd324780e",
   "metadata": {},
   "source": [
    "\n",
    "One common example of a classification problem where precision is the most important metric is spam email detection.\n",
    "\n",
    "Consequences of False Positives: In email classification, a false positive occurs when a legitimate email is incorrectly classified as spam. False positives can have significant negative consequences, such as missing important emails from colleagues, clients, or family members. It can lead to frustration and potentially lost opportunities or information.\n",
    "\n",
    "User Experience: Users generally have low tolerance for false positives. If their important emails are consistently classified as spam, they might lose trust in the email filtering system and may even stop using it. High precision ensures that legitimate emails are rarely marked as spam, improving the user experience.\n",
    "\n",
    "Reducing Annoyance: Users find spam emails annoying and disruptive. High precision helps in reducing the annoyance factor by ensuring that spam emails are accurately detected and filtered out, minimizing the chances of spam emails reaching the inbox.\n",
    "\n",
    "Compliance: In some cases, businesses must comply with regulations regarding email communication, and incorrectly marking legitimate emails as spam can lead to compliance issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aba1e8a-a35b-44d8-a066-cf7e83c2cb8f",
   "metadata": {},
   "source": [
    "Q9. Provide an example of a classification problem where recall is the most important metric, and explain\n",
    "why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200445f0-ca05-41b3-86ff-8c087f800d22",
   "metadata": {},
   "source": [
    "One example of a classification problem where recall is the most important metric is disease detection in the medical field, particularly when dealing with life-threatening or severe diseases.\n",
    "\n",
    "\n",
    "Example: Detecting Cancer\n",
    "\n",
    "Suppose you have a machine learning model designed to classify medical images as either \"cancerous\" or \"non-cancerous\" based on X-ray or MRI scans. In this scenario, recall becomes the most important metric for the following reasons:\n",
    "\n",
    "Consequences of False Negatives: In medical diagnosis, a false negative occurs when the model fails to detect a disease when it is actually present. In the case of cancer detection, a false negative could mean that a patient with cancer is not diagnosed, leading to delayed treatment and potentially allowing the disease to progress to a more advanced and less treatable stage. In such cases, false negatives can have severe consequences, including loss of life or diminished quality of life.\n",
    "\n",
    "Medical Decision-Making: Medical professionals heavily rely on the results of diagnostic tests. If a machine learning model designed for disease detection has low recall, it might miss a significant number of cases, leading to incorrect medical decisions and treatment plans.\n",
    "\n",
    "Patient Safety: High recall is essential for ensuring patient safety. Detecting diseases early is often critical for providing timely and effective treatment. A high recall rate minimizes the chances of missing serious conditions.\n",
    "\n",
    "Reducing False Alarms: While precision is important in medical diagnosis, a high precision might lead to an excessive number of false alarms. These false alarms can cause unnecessary stress and medical procedures for patients. A focus on recall ensures that true positive cases (actual diseases) are not missed while still maintaining a reasonable level of precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad8fc9e-c8ca-4de3-86bf-dcf98e146d76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
